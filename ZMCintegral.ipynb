{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Coded by Jun-Jie Zhang (University of Science and Technology of China) in 06/2019\n",
    "and checked by Hong-Zhong Wu(University of Science and Technology of China).\n",
    "\n",
    "This program is free: you can redistribute it and/or modify it under the terms of \n",
    "the Apache License Version 2.0, January 2004 (http://www.apache.org/licenses/).\n",
    "\n",
    "The program requires python numba and ray to be pre-installed in your \n",
    "GPU-supported computer. \n",
    "\n",
    "'''\n",
    "ZMCIntegral_VERSION = '4.0.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import numba as nb\n",
    "import random\n",
    "import os\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float64\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def gpu_nums():\n",
    "    # return all detected available gpus, int\n",
    "    detected_gpu_id = ray.cluster_resources()['GPU']\n",
    "    return detected_gpu_id\n",
    "\n",
    "def clean_temp():\n",
    "    folder = os.getcwd()+'/multi_temp/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # clean temp file\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def MCkernel(domain, i_batch, my_func, dim, batch_size, num_chunks,\n",
    "             num_points_in_one_chunk, num_chunks_in_one_dimension, num_trials):\n",
    "\n",
    "    '''\n",
    "    Function:\n",
    "        do Monte-Carlo integration on specific GPU device with uniform sampling on \n",
    "        every given chunk domain that the specific GPU device will process. \n",
    "        Remember that the current GPU device will process as possible as 'batch_size'\n",
    "        chunks in each time when chunks' allocation happens.\n",
    "    Parameters:\n",
    "        @domain: domain of the integral, eg: [[a,b],[c,d],...].\n",
    "        @i_batch: current batch index, type:int.\n",
    "        @my_func: user defined function, should be in sting format and the integrand's name must be 'fun'.\n",
    "        @dim: integration dimension.\n",
    "        @batch_size: number of chunks in one allocated process.\n",
    "        @num_chunks: number of total chunks.\n",
    "        @num_points_in_one_chunk: number of samples in one chunk.\n",
    "        @num_chunks_in_one_dimension: number of chunks in every dimension.\n",
    "        @num_trials: the number of independent trials for current chunk.\n",
    "    '''\n",
    "    # have a look at current batch index and node ip.\n",
    "    #print(i_batch)\n",
    "    # execute user defined function as global variable\n",
    "    exec(my_func, globals()) \n",
    "    \n",
    "    # result for accumulating different trials\n",
    "    trial_result = np.zeros([num_trials, batch_size])\n",
    "    # get the small chunks' range, later this array will be used to calculate the volumn of specific integration domain\n",
    "    domain_range = np.array([(domain[j_dim][1] - domain[j_dim][0]) / num_chunks_in_one_dimension for j_dim in range(dim)],\n",
    "                            dtype=np.float64)\n",
    "    # incase the original input is a list like type\n",
    "    domain = np.array(domain, dtype=np.float64)\n",
    "\n",
    "    # this function convert one dimensional index to n dimensional index locally.\n",
    "    @cuda.jit(device=True)\n",
    "    def oneD_to_nD(digit_x, oneD_idx, digit_store):\n",
    "        '''\n",
    "        Principle used here:\n",
    "            oneD_idx = c0*(digit_x**0) +c1*(digit_x**1)+c2*(digit_x**2)+c3*(digit_x**3)+...\n",
    "        Parameters:\n",
    "            @digit_x: the scaled number in one dimension\n",
    "            @oneD_idx: current one dimensional index\n",
    "            @digit_store: store the converted n dimensional index\n",
    "        '''\n",
    "        j_dim_index = 0\n",
    "        digit_store[j_dim_index] = oneD_idx % digit_x\n",
    "        a1 = oneD_idx // digit_x\n",
    "        \n",
    "        # convert to n-dim index\n",
    "        for j_dim in range(dim):\n",
    "            j_dim_index += 1\n",
    "            if a1 != 0.:\n",
    "                digit_store[j_dim + 1] = a1 % digit_x\n",
    "                a1 = a1 // digit_x\n",
    "\n",
    "    @cuda.jit\n",
    "    def integration_kernel(MCresult, num_points_in_one_chunk, num_chunks_in_one_dimension,\n",
    "                           domain, domain_range, batch_size, i_batch, rng_states, num_chunks):\n",
    "\n",
    "        thread_id = cuda.grid(1)\n",
    "        if thread_id < batch_size:\n",
    "            chunk_id = thread_id + i_batch * batch_size\n",
    "\n",
    "            if chunk_id < num_chunks:\n",
    "\n",
    "                # local digits index for each thread\n",
    "                digit_store = cuda.local.array(shape=dim, dtype=nb.int64)\n",
    "                for i_temp in range(dim):\n",
    "                    digit_store[i_temp] = 0\n",
    "\n",
    "                # convert one_dim index to n_dim index\n",
    "                # result will be stored in digit_store\n",
    "                oneD_to_nD(num_chunks_in_one_dimension, chunk_id, digit_store)\n",
    "\n",
    "                # specify the local domain\n",
    "                domain_left = cuda.local.array(shape=dim, dtype=nb.float64)\n",
    "                for j_dim in range(dim):\n",
    "                    domain_left[j_dim] = domain[j_dim][0] + digit_store[j_dim] * domain_range[j_dim]\n",
    "\n",
    "                for i_sample in range(num_points_in_one_chunk):\n",
    "                    # x_tuple: local axis values for each thread\n",
    "                    x_tuple = cuda.local.array(shape=dim, dtype=nb.float64)\n",
    "\n",
    "                    for j_dim in range(dim):\n",
    "                        x_tuple[j_dim] = xoroshiro128p_uniform_float64(rng_states, thread_id) * domain_range[j_dim] + domain_left[j_dim]\n",
    "\n",
    "                    # feed in values to user defined function, \n",
    "                    # and add all points' corresponding results in one chunk\n",
    "                    cuda.atomic.add(MCresult, thread_id, fun(x_tuple))\n",
    "\n",
    "    # Configure the threads and blocks\n",
    "    threadsperblock = 32\n",
    "    blockspergrid = (batch_size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    for i_trial in range(num_trials):\n",
    "        rng_states = create_xoroshiro128p_states(threadsperblock * blockspergrid, \n",
    "                                             seed=random.sample(range(0,100000),1)[0])\n",
    "        rng_states = cuda.to_device(rng_states)\n",
    "\n",
    "        MCresult = cuda.device_array(batch_size, dtype=np.float64)\n",
    "        # Start the kernel\n",
    "        integration_kernel[blockspergrid, threadsperblock](MCresult, num_points_in_one_chunk, num_chunks_in_one_dimension,\n",
    "                                                           domain, domain_range, batch_size, i_batch, rng_states, num_chunks)\n",
    "\n",
    "        # volumn of the domain divided by number of points in one chunk\n",
    "        volumn = np.prod(domain_range) / num_points_in_one_chunk\n",
    "        \n",
    "        MCresult = MCresult.copy_to_host()\n",
    "\n",
    "        MCresult = volumn * MCresult\n",
    "\n",
    "        trial_result[i_trial] = MCresult\n",
    "        \n",
    "    print(\"current batch: {0}, trial number: {1}\".format(i_batch, num_trials))\n",
    "\n",
    "    return trial_result\n",
    "\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def MCkernel_mean(trial_result):\n",
    "    return np.mean(trial_result,0)\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def MCkernel_std(trial_result):\n",
    "    return np.std(trial_result,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCintegral():\n",
    "    def __init__(self, my_func = None, domain = None,head_node_address = None,\n",
    "                 num_trials = 5, depth = 2, sigma_multiplier = 5, num_points_in_one_chunk = 10000,\n",
    "                 num_chunks_in_one_dimension = 4, batch_size = 200000):\n",
    "\n",
    "        '''\n",
    "        Parameters:\n",
    "            @my_func: user defined multidimensional function, type:string, the integrand must have name \"fun\"\n",
    "            @domain: integration domain, type:list/numpy_array, eg [[0,1]] or [[0,1],[0,1]]\n",
    "            @head_node_address: head node address with port, type: str \n",
    "            @num_trials: number of independent trials for every chunk, type:int, Default:2\n",
    "            @depth: depth of heuristic tree search, type:int, Default:2\n",
    "            @sigma_multiplier: recalculate the grid if `stddev` larger than `sigma_mean + sigma_multiplier * sigma`, \n",
    "                                    type:float, Default:5\n",
    "            @num_points_in_one_chunk: number of samples in one chunk, type: int\n",
    "            @num_chunks_in_one_dimension: number of chunks in every dimension.\n",
    "            @batch_size: number of chunks in one allocated process.\n",
    "        '''\n",
    "\n",
    "        # clean temporary file\n",
    "        clean_temp()\n",
    "        \n",
    "        # specify head node address\n",
    "        if head_node_address == None:\n",
    "            raise AssertionError(\"You must provide head node address with port.\")\n",
    "            \n",
    "        ray.shutdown()\n",
    "        ray.init(redis_address = head_node_address)\n",
    "\n",
    "        # specify available number of gpus\n",
    "        self.num_gpus = int(ray.get(gpu_nums.remote()))\n",
    "        print(\"total number of GPUs: \", self.num_gpus)\n",
    "      \n",
    "        # number of trials\n",
    "        self.num_trials = num_trials\n",
    "        \n",
    "        # depth of the digging\n",
    "        self.depth = depth\n",
    "\n",
    "        # recalculate the grid if `stddev` larger than `sigma_mean + sigma_multiplier * sigma`\n",
    "        self.sigma_multiplier = sigma_multiplier\n",
    "        \n",
    "        # detect if domain is in right form\n",
    "        if domain == None:\n",
    "            raise AssertionError(\"Please enter a domain\")\n",
    "        for temp in domain:\n",
    "            if len(temp) != 2:\n",
    "                raise AssertionError(\"Domain is incorrect\")\n",
    "            if temp[1] < temp[0]:\n",
    "                raise AssertionError(\"Domain [a,b] should satisfy b>a\")\n",
    "                \n",
    "        # initial domain\n",
    "        self.initial_domain = domain\n",
    "                \n",
    "        # integrating dimension\n",
    "        self.dim = len(domain)\n",
    "        \n",
    "        # user defined function\n",
    "        self.my_func = my_func\n",
    "        \n",
    "        # number of chunks for each gpu to evaluate in each gpu allocation process\n",
    "        # this is the number of threads\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # number of sampling points in one chunk \n",
    "        # this number should not be very large otherwise the gpu will yield overflow\n",
    "        self.num_points_in_one_chunk = num_points_in_one_chunk\n",
    "            \n",
    "        # number of chunks in one dimension\n",
    "        self.num_chunks_in_one_dimension = num_chunks_in_one_dimension\n",
    "\n",
    "        # number of chunks\n",
    "        self.num_chunks = self.num_chunks_in_one_dimension ** self.dim\n",
    "        \n",
    "        # number of batches, i.e., the required total number of gpu allocation\n",
    "        self.num_batches = self.num_chunks // self.batch_size + 1\n",
    "        \n",
    "    def evaluate(self):\n",
    "        MCresult = self.stratified_sampling_iteration(self.initial_domain, 0)        \n",
    "        return MCresult\n",
    "    \n",
    "    def stratified_sampling_iteration(self, domain, depth):\n",
    "        depth += 1\n",
    "        MCresult_chunks, MCresult_std_chunks, large_std_chunk_id = self.MCevaluate(domain)\n",
    "        print('{} hypercube(s) need(s) to be recalculated, to save time, try increasing sigma_multiplier.'.format(len(large_std_chunk_id)))\n",
    "        if depth < self.depth:\n",
    "            for chunk_id in large_std_chunk_id:\n",
    "                # domain of this chunk\n",
    "                domain_next_level = self.chunk_domian(chunk_id, domain)\n",
    "                \n",
    "                # iteration\n",
    "                MCresult_chunks[chunk_id], MCresult_std_chunks[chunk_id] = self.stratified_sampling_iteration(domain_next_level, depth)\n",
    "                \n",
    "        # Stop digging if there are no more large stddev chunk even the required digging depth is not reached\n",
    "        if len(large_std_chunk_id) == 0:\n",
    "            return np.sum(MCresult_chunks, 0), np.sqrt(np.sum(MCresult_std_chunks**2))\n",
    "\n",
    "        return np.sum(MCresult_chunks, 0), np.sqrt(np.sum(MCresult_std_chunks**2))\n",
    "\n",
    "    \n",
    "    def MCevaluate(self, domain):\n",
    "\n",
    "        '''\n",
    "        Monte Carlo integration.\n",
    "        Parameters:\n",
    "            @domain: the integration domain, type:list or numpy_array.\n",
    "        '''\n",
    "        \n",
    "        # result for accumulating data\n",
    "        MCresult = []\n",
    "        MCresult_std = []\n",
    "        \n",
    "        # loop through all gpus\n",
    "        for i_batch in range(self.num_batches):\n",
    "            \n",
    "            # distribute calculation\n",
    "            trial_result = MCkernel.remote(domain, i_batch, self.my_func, self.dim,\n",
    "                                           self.batch_size, self.num_chunks, self.num_points_in_one_chunk,\n",
    "                                           self.num_chunks_in_one_dimension, self.num_trials)\n",
    "            \n",
    "            MCresult.append(MCkernel_mean.remote(trial_result))\n",
    "            MCresult_std.append(MCkernel_std.remote(trial_result))\n",
    "\n",
    "        # get data back to head node\n",
    "        MCresult = np.concatenate(ray.get(MCresult))\n",
    "        MCresult_std = np.concatenate(ray.get(MCresult_std))\n",
    "       \n",
    "        # find out the index of chunks that have very large stddev\n",
    "        threshold = np.mean(MCresult_std) + self.sigma_multiplier * np.std(MCresult_std)\n",
    "        large_std_chunk_id = np.nonzero(MCresult_std >= threshold)[0]\n",
    "       \n",
    "        return MCresult, MCresult_std, large_std_chunk_id\n",
    "\n",
    "    \n",
    "    def chunk_domian(self, chunk_id, original_domain):\n",
    "\n",
    "        '''\n",
    "        Return:\n",
    "            domain of integration in this chunk.\n",
    "        Parameters:\n",
    "            @chunk_id: current chunk id, type:int.\n",
    "            @original_domain: the domain of the previous original integration.\n",
    "        '''\n",
    "        \n",
    "        chunk_id_d_dim = np.unravel_index(chunk_id, [self.num_chunks_in_one_dimension for _ in range(self.dim)])\n",
    "        domain_range = np.array([(original_domain[idim][1] - original_domain[idim][0]) / self.num_chunks_in_one_dimension for idim in range(self.dim)], dtype=np.float64)\n",
    "        domain_left = np.array([original_domain[idim][0] + chunk_id_d_dim[idim] * domain_range[idim] for idim in range(self.dim)], dtype=np.float64)\n",
    "        current_domain = [[domain_left[i], domain_left[i] + domain_range[i]] for i in range(self.dim)]\n",
    "        return current_domain    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of GPUs:  4\n",
      "\u001b[2m\u001b[36m(pid=28535)\u001b[0m current batch: 3, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28536)\u001b[0m current batch: 1, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28537)\u001b[0m current batch: 2, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28538)\u001b[0m current batch: 0, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28535)\u001b[0m current batch: 4, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28536)\u001b[0m current batch: 5, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28537)\u001b[0m current batch: 6, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28538)\u001b[0m current batch: 7, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28536)\u001b[0m current batch: 9, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28535)\u001b[0m current batch: 8, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28537)\u001b[0m current batch: 10, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28538)\u001b[0m current batch: 11, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28536)\u001b[0m current batch: 12, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28535)\u001b[0m current batch: 13, trial number: 5\n",
      "\u001b[2m\u001b[36m(pid=28537)\u001b[0m current batch: 14, trial number: 5\n",
      "137 hypercube(s) need(s) to be recalculated, to save time, try increasing sigma_multiplier.\n",
      "result = -48.473634536766795    std = 1.9871465878803765\n",
      "38.989293813705444\n"
     ]
    }
   ],
   "source": [
    "# record total time, experiment 1\n",
    "import time\n",
    "a=time.time()\n",
    "# user defined function\n",
    "fun = \"\"\" \n",
    "import math\n",
    "# define a device function that should be used by cuda kernel\n",
    "@cuda.jit(device=True)\n",
    "def fun(x):\n",
    "    return math.sin(x[0]+x[1]+x[2]+x[3]+x[4]+x[5]+x[6])\n",
    "\"\"\"\n",
    "depth = 1\n",
    "sigma_multiplier = 5\n",
    "num_trials = 5\n",
    "num_chunks_in_one_dimension = 12\n",
    "\n",
    "MC = MCintegral(my_func = fun, domain = [[0,10],[0,10],[0,10],[0,10],[0,10],[0,10]], head_node_address = \"210.45.78.43:6789\",\n",
    "                depth = depth, sigma_multiplier = sigma_multiplier, num_trials = num_trials,\n",
    "                num_chunks_in_one_dimension = num_chunks_in_one_dimension)\n",
    "\n",
    "# obtaining the result\n",
    "result = MC.evaluate()\n",
    "\n",
    "# print the formatted result\n",
    "print('result = %s    std = %s' % (result[0], result[1]))\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of GPUs:  4\n",
      "\u001b[2m\u001b[36m(pid=28538)\u001b[0m current batch: 0, trial number: 51 hypercube(s) need(s) to be recalculated, to save time, try increasing sigma_multiplier.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=28536)\u001b[0m current batch: 0, trial number: 5\n",
      "1 hypercube(s) need(s) to be recalculated, to save time, try increasing sigma_multiplier.\n",
      "\u001b[2m\u001b[36m(pid=28538)\u001b[0m current batch: 0, trial number: 5\n",
      "1 hypercube(s) need(s) to be recalculated, to save time, try increasing sigma_multiplier.\n",
      "252 hypercube(s) need(s) to be recalculated, to save time, try increasing sigma_multiplier.\n",
      "result = 0.9995699473746997    std = 0.001296422529913424\n",
      "9.056021451950073\n",
      "\u001b[2m\u001b[36m(pid=28537)\u001b[0m current batch: 0, trial number: 5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a=time.time()\n",
    "# user defined function\n",
    "fun = \"\"\" \n",
    "import math\n",
    "# define a device function that should be used by cuda kernel\n",
    "@cuda.jit(device=True)\n",
    "def fun(x):\n",
    "    return ((1/math.sqrt(2 * math.pi * 0.0001))**9) * math.exp(-(x[0]**2+x[1]**2+x[2]**2+x[3]**2+x[4]**2+x[5]**2+x[6]**2+x[7]**2+x[8]**2)/(2*0.0001))\n",
    "\"\"\"\n",
    "depth = 4\n",
    "sigma_multiplier = 5\n",
    "num_trials = 5\n",
    "num_chunks_in_one_dimension = 3\n",
    "\n",
    "MC = MCintegral(my_func = fun, domain = [[-1,1],[-1,1],[-1,1],[-1,1],[-1,1],[-1,1],[-1,1],[-1,1],[-1,1]], \n",
    "                head_node_address = \"210.45.78.43:6789\",depth = depth, sigma_multiplier = sigma_multiplier, \n",
    "                num_trials = num_trials, num_chunks_in_one_dimension = num_chunks_in_one_dimension)\n",
    "\n",
    "# obtaining the result\n",
    "result = MC.evaluate()\n",
    "\n",
    "# print the formatted result\n",
    "print('result = %s    std = %s' % (result[0], result[1]))\n",
    "print(time.time()-a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
